\subsubsection{Worst-Case Analyse}
\label{bpr:effizienz:theorie}

Die Worst-Case Analyse des Algorithmus kann für beide Phasen separat erfolgen, da die asymptotische Schranke nur durch die schlechtere Schranke der beiden Phasen festgelegt wird. Wir betrachten daher zunächst die Speicher- und Laufzeitkomplexität der ersten Phase, bevor wir die zweite Phase auf ähnliche Art und Weise untersuchen. Für die angegebenen Laufzeitschranken gibt die Größe \(n\) dabei immer die Länge des Eingabestrings an.
\paragraph*{Phase 1}
In der ersten Phase des Algorithmus wird die initiale Einteilung der Suffixe in Level-\(d\)-Buckets bestimmt. Gemäß der Beschreibung von Schürmann und Stoye \cite[Abschnitt 3.3]{saca:2} wird dazu eine Tabelle \bkt (s.~Abbildung \ref{fig:bkt}) angelegt. Bedingt durch die Größe des Alphabets und die Länge \(d\) der Präfixe fällt auf, dass die Tabelle Speicher in der Größe von \(\O(|\Sigma|^d)\) benötigt. Unter der Annahme, dass die Allokation von Speicher der Größe \(n\) einen Laufzeitaufwand von \(\Theta(n)\) verursacht, bildet die Speicherkomplexität zugleich eine untere Schranke für die Laufzeitkomplexität. Dennoch geben Schürmann und Stoye in ihrer Analyse eine Worst-Case Laufzeit von \(\O(n)\) für die erste Phase an \cite[Kapitel 3.2]{saca:2}. Um zumindest in der Theorie eine lineare Laufzeit in Abhängigkeit von \(n\) zu garantieren, kann angenommen werden, dass \(|\Sigma|\) und \(d\) konstant oder zumindest unabhängig von \(n\) sind. Falls \(d\) allerdings von \(n\) abhängig gewählt wird, so sind einige andere Faktoren entscheidend, auf welche im Folgenden eingegangen wird.\par
Zunächst ist zu beachten, dass nicht der gesamte allokierte Speicher auch verwendet werden muss: Die Verarbeitung der Eingabe erfolgt laut Dokumentation des Algorithmus in dieser Phase in einer konstanten Anzahl von Iterationen über den Eingabestring\footnote{Laut Schürmann und Stoye \cite[Kapitel~3.3]{saca:2} besteht die erste Phase von \bpr lediglich aus drei Iterationen über den Eingabestring, was eine lineare Laufzeitschranke zur Folge hat. Bei genauerer Betrachtung der im Rahmen der Publikation veröffentlichten Implementierung (abrufbar unter \url{http://bibiserv.techfak.uni-bielefeld.de/bpr/}) fällt allerdings auf, dass tatsächlich auch eine vollständige Iteration über das Array \bkt stattfindet, welche die Laufzeitschranke auf \(\O(|\Sigma|^d)\) anhebt. \(d\) wird aber im weiteren Verlauf des Papers als logarithmisch abhängig von \(|\Sigma|\) gewählt. Diese Iteration wird jedoch weder in der Beschreibung des Algorithmus erwähnt, noch in der zugehörigen Analyse berücksichtigt.} \inputtext bzw. \inputtextplus. Obwohl das Array \bkt Platz für alle möglichen Präfixe der Länge \(d\) bietet, müssen nur die Einträge verwendet werden, zu denen tatsächlich entsprechende Teilstrings in \inputtextplus existieren. Die Anzahl solcher Präfixe ist allerdings durch \(\O(n\)) beschränkt, woraus eine lineare Laufzeit resultiert, sofern es die Implementierung erlaubt, Speicher beliebiger Größe in konstanter Zeit zu allokieren.\par
Alternativ ist auch die Verwendung einer geeigneten Hashtabelle denkbar, um den exponentiellen Speicherbedarf zu vermeiden. Ein solcher Ansatz bietet -- wenn auch im ursprünglichen Algorithmus nicht vorgesehen -- für große Alphabete schon bei kleiner Konstante \(d\) die einzige Möglichkeit, den Algorithmus auf gängiger Hardware angemessen speicherschonend betreiben zu können.
\paragraph*{Phase 2}
Für die asymptotische Laufzeit der zweiten Phase wurde von Schürmann und Stoye nur eine auf groben Abschätzungen basierende Schranke von \(\O(n^2)\) angegeben \cite[Kapitel~3.2]{saca:2}. Die entsprechende Analyse beruht auf der Annahme, dass in jeder Ebene der Rekursion insgesamt höchstens \(n\) Suffixe sortiert werden müssen, was eine Laufzeit von \(\O(n \log n)\) zur Folge habe. Da die Anzahl der Rekursionsebenen offensichtlich durch \(\frac n d\) beschränkt ist (bedingt durch die Erhöhung von \offset um \(d\) pro Ebene), sei die Laufzeit der gesamgen Phase durch \(\O(\frac n d \cdot n \log n)\) beschränkt. Es wird außerdem angegeben, dass eine Schranke von \(\O(n^2)\) erreicht werden kann, wenn \(d = \log n\) gesetzt wird.\par\smallskip
Wir wollen uns diese Laufzeitschranke genauer ansehen. Zunächst fällt auf, dass für den Sortiervorgang in Phase 2 die Algorithmen \emph{Insertionsort} und \emph{Quicksort} verwendet werden. \emph{Insertionsort} hat bekanntermaßen sowohl im besten Fall, als auch im durchschnittlichen Fall eine Laufzeit von \(\O(n^2)\). Da dieser Algorithmus allerdings nur für Buckets der Größe 15 oder kleiner verwendet wird, ist in diesem Fall \(n\) durch 15 nach oben beschränkt, woraus sogar eine konstante Laufzeit resultiert. Interessanter ist die Analyse bei \emph{Quicksort}: Zwar beträgt die Laufzeit dieses Verfahrens im durchschnittlichen Fall \(\O(n \log n)\), im schlechtesten Fall kann die Laufzeit allerdings auch bis auf \(\O(n^2)\) ansteigen. In einer Worst-Case Analyse muss also streng genommen eine quadratische Laufzeitschranke für den Sortiervorgang angenommen werden. Die von Schürmann und Stoye angenommene asymptotische Schranke von \(\O(n \log n)\) kann tatsächlich erreicht werden, wenn für den Sortiervorgang \emph{Mergesort} oder \emph{Heapsort} verwendet wird. Da es in der Praxis dennoch vorteilhaft sein kann, \emph{Quicksort} zu verwenden \cite[Tabelle~1]{quicksort}, ist es hier vertretbar, die Sortiervorgänge mit diesem Algorithmus durchzuführen und anzumerken, dass eine asymptotische Laufzeitschranke von \(\O(n \log n)\) zumindest mit alternativen Sortieralgorithmen \emph{möglich} ist. Wir gehen daher auch in der weiteren Analyse davon aus, dass Sortieren in \(\O(n \log n)\) möglich ist.\par
Hervorzuheben ist auch die Wahl von \(d = \log n\), um insgesamt eine Laufzeit von \(\O(n^2)\) zu erhalten. Den Parameter \(d\) in Abhängigkeit von \(n\) zu wählen, ist zwar grundsätzlich möglich, steht aber im Gegensatz zur in Phase 1 aufgestellten Annahme, \(d\) sei konstant. Betrachten wir nur konstante Werte für \(d\), so fällt der Faktor \(\frac 1 d\) in \(\O(\frac n d \cdot n \log n)\) weg und wir erhalten eine Worst-Case Laufzeit von \(\O(n^2\log n)\).\par\smallskip
Obwohl es mit der oben angewandten Methode zur Abschätzung der Laufzeit, welche sich auf die Tiefe der Rekursion und den maximalen Aufwand pro Ebene bezieht, scheinbar nicht möglich ist, kleinere obere Schranken als \(\O(n^2 \log n)\) zu finden, konnte noch keine einfache Beispieleingabe gefunden werden, die tatsächlich eine derart hohe Laufzeit verursacht. Die größte Schwierigkeit dabei ist, dass der Algorithmus vergleichsweise undurchsichtig die Abhängigkeiten zwischen den Suffixen verwendet.
\paragraph*{Gesamter Algorithmus}
In den beiden vorangegangenen Abschnitten wurde die Laufzeit der beiden Phasen einzeln analysiert. Eine Laufzeitschranke für den Gesamten Algorithmus ergibt sich durch das Maximum beider Komponenten. Da es aber für eine präzise Angabe notwendig ist, die Wahl des Parameters \(d\) festzulegen, betrachten wir zuerst einige Optionen. Im Vorfeld wurde bereits diskutiert, welche Auswirkungen auf die Laufzeit die Wahl von \(d = \log n\) im Gegensatz zu einem konstanten Wert für \(d\) hat. Diese Unterschiede zeigt Tabelle~\ref{table:time_bounds}.
\begin{table}[h]
	\centering
    \resizebox{\textwidth}{!}{
        \begin{tabular}{|l|l|l|l|}
            \hline
                Phase & \(d\) konstant & \(d = \log n\) & \(d = \log_{|\Sigma|}n\) \\
                \hline
                \hline
                Phase 1 & \(\O(|\Sigma|^d)\) & \(\O(|\Sigma|^{\log n}) = \O(n^{\log |\Sigma|})\) & \(\O(|\Sigma|^{\log_{|\Sigma|} n}) = \O(n^{\log_{|\Sigma|} |\Sigma|}) = \O(n)\) \\
                Phase 2 & \(\O(n^2 \log n)\) & \(\O(n^2)\) & \(\O\left(\frac{n^2 \log n}{\log_{|\Sigma|}n}\right) = \O(n^2 \log |\Sigma|)\) \\
                \hline
                Gesamt & \(\O(|\Sigma|^d + n^2 \log n)\) & \(\O(n^{\max\{2, \log |\Sigma|\}})\) & \(\O(n^2 \log |\Sigma|)\)\\
                \hline
        \end{tabular}
    }
    \vspace{1ex}
    \caption{Laufzeitschranken für \bpr in Abhängigkeit von \(d\)}
    \label{table:time_bounds}
\end{table}
Für einen konstanten Wert \(d\) haben wir bereits in den beiden teilweisen Analysen gesehen, dass die Laufzeit polynomiell in \(|\Sigma|\) und \(n\) ist. Allerdings ist festzustellen, dass sich bereits für kleine \(d\) eine sehr hohe reale Laufzeit ergeben kann. Für den Fall \(d = \log n\), welcher von Schürmann und Stoye gewählt wurde, ist die Laufzeit zwar für feste Alphabetgrößen \(|\Sigma|\) polynomiell in \(n\). Für allgemeine Alphabete jedoch ist die Laufzeit nicht polynomiell.\par
Ein möglicher Ansatz um eine in \(n\) und \(|\Sigma|\) polynomielle Laufzeitschranke zu erreichen kann gefunden werden, wenn \(d\) in Abhängigkeit der Alphabetgröße \(|\Sigma|\) und der Eingabelänge \(n\) als \(d = \log_{|\Sigma|} n\) gewählt wird. Da auf diese Option bisher noch nicht eingegangen wurde, sind in Tabelle~\ref{table:time_bounds} auch die separaten Schranken für die erste und zweite Phase angegeben. Der Parameter \(d\) ist hier so gewählt, dass das exponentielle Wachstum in Phase 1, wie es für \(d = \log n\) auftritt, vermieden werden kann, wodurch dort eine asymptotische Laufzeit von \(\O(n)\) erreicht wird. Die Laufzeit für die zweite Phase ist zwar etwas schlechter als für \(d = \log n\), da aber \(|\Sigma| \leq n\) gilt, wird hier zumindest eine bessere Laufzeit erzielt, als für ein konstantes \(d\).
